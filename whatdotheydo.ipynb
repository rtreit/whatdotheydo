{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import random\n",
    "import pandas as pd\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "credential = AzureCliCredential()\n",
    "\n",
    "token = credential.get_token(\"https://app.vssps.visualstudio.com/.default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ADO_ORG_URL = f'https://dev.azure.com/{os.getenv(\"ADO_ORG\")}'\n",
    "ADO_PROJECT = os.getenv(\"ADO_PROJECT\")\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "START_DATE = datetime(2024, 1, 1)  # inclusive\n",
    "END_DATE = datetime(2025, 1, 1)  # exclusive\n",
    "FILTER_USER = False  # set to False to get commit data for all users\n",
    "USER = \"example@example.com\"\n",
    "\n",
    "BASE_URL = f\"https://microsoft.visualstudio.com/{ADO_PROJECT}/_apis/git/repositories\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Host\": \"microsoft.visualstudio.com\",\n",
    "    \"User-Agent\": \"python/3.11.8 (Windows-10-10.0.22635-SP0) msrest/0.7.1 azure-devops/5.1.0b4 devOpsCli/1.0.1\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept\": \"application/json;api-version=5.0\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Content-Type\": \"application/json; charset=utf-8\",\n",
    "    \"Authorization\": f\"Bearer {token.token}\",\n",
    "}\n",
    "\n",
    "\n",
    "async def fetch_repositories(session: aiohttp.ClientSession) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch all repositories and return a dict mapping repo_id to repo_name.\n",
    "    \"\"\"\n",
    "    async with session.get(BASE_URL, headers=HEADERS) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            repos = data.get(\"value\", [])\n",
    "            return {\n",
    "                repo[\"id\"]: repo[\"name\"]\n",
    "                for repo in repos\n",
    "                if repo[\"isDisabled\"] == False\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Failed to fetch repositories. Status code: {response.status}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "def summarize_commit(commit_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Given a single commit JSON structure from Azure DevOps,\n",
    "    produce a summary of key information.\n",
    "    \"\"\"\n",
    "    commit_id = commit_json.get(\"commitId\", \"\")\n",
    "    author_info = commit_json.get(\"author\", {})\n",
    "    author_name = author_info.get(\"name\", \"\")\n",
    "    author_email = author_info.get(\"email\", \"\")\n",
    "    commit_date = author_info.get(\"date\", \"\")\n",
    "    comment = commit_json.get(\"comment\", \"\")\n",
    "\n",
    "    total_changed_files = []\n",
    "    changes = commit_json.get(\"changes\", []).get(\"changes\", [])\n",
    "    for change in changes:\n",
    "        if change.get(\"item\", {}).get(\"gitObjectType\") == \"blob\":\n",
    "            filename = change.get(\"item\", {}).get(\"path\", \"\").split(\"/\")[-1]\n",
    "            total_changed_files.append(filename)\n",
    "    summary = {\n",
    "        \"commitId\": commit_id,\n",
    "        \"authorName\": author_name,\n",
    "        \"authorEmail\": author_email,\n",
    "        \"date\": commit_date,\n",
    "        \"comment\": comment,\n",
    "        \"files\": total_changed_files,\n",
    "        \"changedFilesCount\": len(total_changed_files),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "async def fetch_with_retries(\n",
    "    session, url, headers, retries=3, delay=2, backoff=2, timeout=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform an HTTP GET request with retries in case of transient failures.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, headers=headers, timeout=timeout) as response:\n",
    "                if response.status == 200:\n",
    "                    return await response.json()\n",
    "                else:\n",
    "                    print(f\"Attempt {attempt + 1} failed with status {response.status}\")\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "\n",
    "        if attempt < retries - 1:\n",
    "            await asyncio.sleep(delay)\n",
    "            delay *= backoff  # Exponential backoff\n",
    "    raise Exception(f\"Failed to fetch {url} after {retries} retries\")\n",
    "\n",
    "\n",
    "async def fetch_commits_for_day(\n",
    "    session: aiohttp.ClientSession,\n",
    "    repo_id: str,\n",
    "    repo_name: str,\n",
    "    day_start: datetime,\n",
    "    day_end: datetime,\n",
    "    filter_user: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    from_str = day_start.isoformat() + \"Z\"\n",
    "    to_str = day_end.isoformat() + \"Z\"\n",
    "\n",
    "    email_filter = USER\n",
    "\n",
    "    if filter_user:\n",
    "        url = (\n",
    "            f\"{BASE_URL}/{repo_id}/commits?searchCriteria.fromDate={from_str}\"\n",
    "            f\"&searchCriteria.toDate={to_str}\"\n",
    "            f\"&searchCriteria.$top=1000\"\n",
    "            f\"&searchCriteria.author={email_filter}&api-version=7.1\"\n",
    "        )\n",
    "    else:\n",
    "        url = (\n",
    "            f\"{BASE_URL}/{repo_id}/commits?searchCriteria.fromDate={from_str}\"\n",
    "            f\"&searchCriteria.toDate={to_str}&searchCriteria.$top=1000&api-version=7.1\"\n",
    "        )\n",
    "\n",
    "    all_commits = []\n",
    "    while url:\n",
    "        try:\n",
    "            data = await fetch_with_retries(session, url, HEADERS)\n",
    "            batch_commits = data.get(\"value\", [])\n",
    "            all_commits.extend(batch_commits)\n",
    "\n",
    "            # Parse 'next' URL from Link header if present\n",
    "            next_url = None\n",
    "            link_header = data.get(\"Link\", \"\")\n",
    "            if link_header and 'rel=\"next\"' in link_header:\n",
    "                parts = link_header.split(\";\")\n",
    "                if parts:\n",
    "                    first_part = parts[0].strip()\n",
    "                    if first_part.startswith(\"<\") and first_part.endswith(\">\"):\n",
    "                        next_url = first_part[1:-1]\n",
    "\n",
    "            url = next_url if next_url and next_url != url else None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Failed to fetch commits for repository '{repo_name}' (ID: {repo_id}) \"\n",
    "                f\"on {day_start.date()}. Error: {e}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # Summarize commits\n",
    "    commit_summaries = []\n",
    "    for commit in all_commits:\n",
    "        commit_id = commit.get(\"commitId\")\n",
    "        commit_url = commit.get(\"url\")\n",
    "\n",
    "        if commit_url and commit_id:\n",
    "            try:\n",
    "                commit_data = await fetch_with_retries(session, commit_url, HEADERS)\n",
    "                changes_url = (\n",
    "                    commit_data.get(\"_links\", {}).get(\"changes\", {}).get(\"href\")\n",
    "                )\n",
    "                if changes_url:\n",
    "                    changes_data = await fetch_with_retries(\n",
    "                        session, changes_url + \"?includeDiff=true\", HEADERS\n",
    "                    )\n",
    "                    commit_data[\"changes\"] = changes_data\n",
    "\n",
    "                commit_data[\"summary\"] = summarize_commit(commit_data)\n",
    "                commit_data[\"summary\"][\"repoName\"] = repo_name\n",
    "                commit_summaries.append(commit_data[\"summary\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch commit details for {commit_id}: {e}\")\n",
    "\n",
    "    if commit_summaries:\n",
    "        return pd.DataFrame(commit_summaries)\n",
    "    else:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"commitId\", \"authorName\", \"comment\", \"repoName\", \"date\"]\n",
    "        )\n",
    "\n",
    "\n",
    "async def fetch_commits_for_repo(\n",
    "    session: aiohttp.ClientSession,\n",
    "    repo_id: str,\n",
    "    repo_name: str,\n",
    "    start_date: datetime,\n",
    "    end_date: datetime,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Orchestrate day-by-day async calls to fetch commits for an entire date range.\n",
    "    Returns a combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    tasks = []\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=1)\n",
    "        task = asyncio.create_task(\n",
    "            fetch_commits_for_day(\n",
    "                session,\n",
    "                repo_id,\n",
    "                repo_name,\n",
    "                current_date,\n",
    "                next_date,\n",
    "                filter_user=FILTER_USER,\n",
    "            )\n",
    "        )\n",
    "        tasks.append(task)\n",
    "        current_date = next_date\n",
    "\n",
    "    daily_dfs = await asyncio.gather(*tasks)\n",
    "\n",
    "    if daily_dfs:\n",
    "        return pd.concat(daily_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    repo_id_name_map = await fetch_repositories(session)\n",
    "    all_repo_ids = list(repo_id_name_map.keys())\n",
    "    print(f\"Found {len(all_repo_ids)} repositories.\")\n",
    "\n",
    "    tasks = [\n",
    "        fetch_commits_for_repo(\n",
    "            session, repo_id, repo_id_name_map[repo_id], START_DATE, END_DATE\n",
    "        )\n",
    "        for repo_id in all_repo_ids\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    data = pd.concat(results)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = data.infer_objects()\n",
    "    data[\"changedFilesCount\"] = data[\"changedFilesCount\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [file for sublist in data[\"files\"] for file in sublist]\n",
    "\n",
    "unique_files = set(all_files)\n",
    "\n",
    "distinct_file_count = len(unique_files)\n",
    "\n",
    "print(\"Total distinct files:\", distinct_file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exploded = data.explode(\"files\")\n",
    "\n",
    "distinct_files_per_repo = (\n",
    "    data_exploded.groupby(\"repoName\")\n",
    "    .agg(distinctFileCount=(\"files\", \"nunique\"))\n",
    "    .sort_values(by=\"distinctFileCount\", ascending=False)\n",
    ")\n",
    "\n",
    "distinct_files_per_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"repoName\").agg(\n",
    "    {\"commitId\": \"nunique\", \"changedFilesCount\": \"sum\"}\n",
    ").sort_values(by=\"commitId\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([\"authorName\"]).agg(\n",
    "    {\"changedFilesCount\": \"sum\", \"commitId\": \"nunique\", \"repoName\": \"nunique\"}\n",
    ").sort_values(\"commitId\", ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

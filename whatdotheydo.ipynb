{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import random\n",
    "import pandas as pd\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ADO_ORG_URL = f'https://dev.azure.com/{os.getenv(\"ADO_ORG\")}'\n",
    "ADO_PAT = os.getenv(\"ADO_PAT\")\n",
    "ADO_PROJECT = os.getenv(\"ADO_PROJECT\")\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "START_DATE = datetime(2024, 1, 1)  # inclusive\n",
    "END_DATE = datetime(2024, 12, 31)  # exclusive\n",
    "FILTER_USER = False  # set to False to get commit data for all users\n",
    "USER = \"example@example.com\"\n",
    "\n",
    "BASE_URL = f\"https://microsoft.visualstudio.com/{ADO_PROJECT}/_apis/git/repositories\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Host\": \"microsoft.visualstudio.com\",\n",
    "    \"User-Agent\": \"python/3.11.8 (Windows-10-10.0.22635-SP0) msrest/0.7.1 azure-devops/5.1.0b4 devOpsCli/1.0.1\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept\": \"application/json;api-version=5.0\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Content-Type\": \"application/json; charset=utf-8\",\n",
    "    \"Authorization\": f'Basic {base64.b64encode(f\":{ADO_PAT}\".encode()).decode()}',\n",
    "}\n",
    "\n",
    "\n",
    "async def fetch_repositories(session: aiohttp.ClientSession) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch all repositories and return a dict mapping repo_id to repo_name.\n",
    "    \"\"\"\n",
    "    async with session.get(BASE_URL, headers=HEADERS) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            repos = data.get(\"value\", [])\n",
    "            return {\n",
    "                repo[\"id\"]: repo[\"name\"]\n",
    "                for repo in repos\n",
    "                if repo[\"isDisabled\"] == False\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Failed to fetch repositories. Status code: {response.status}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "def summarize_commit(commit_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Given a single commit JSON structure from Azure DevOps,\n",
    "    produce a summary of key information.\n",
    "    \"\"\"\n",
    "    commit_id = commit_json.get(\"commitId\", \"\")\n",
    "    author_info = commit_json.get(\"author\", {})\n",
    "    author_name = author_info.get(\"name\", \"\")\n",
    "    author_email = author_info.get(\"email\", \"\")\n",
    "    commit_date = author_info.get(\"date\", \"\")\n",
    "    comment = commit_json.get(\"comment\", \"\")\n",
    "\n",
    "    total_changed_files = []\n",
    "    changes = commit_json.get(\"changes\", []).get(\"changes\", [])\n",
    "    for change in changes:\n",
    "        if change.get(\"item\", {}).get(\"gitObjectType\") == \"blob\":\n",
    "            filename = change.get(\"item\", {}).get(\"path\", \"\").split(\"/\")[-1]\n",
    "            total_changed_files.append(filename)\n",
    "    summary = {\n",
    "        \"commitId\": commit_id,\n",
    "        \"authorName\": author_name,\n",
    "        \"authorEmail\": author_email,\n",
    "        \"date\": commit_date,\n",
    "        \"comment\": comment,\n",
    "        \"files\": total_changed_files,\n",
    "        \"changedFilesCount\": len(total_changed_files),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "async def fetch_commits_for_day(\n",
    "    session: aiohttp.ClientSession,\n",
    "    repo_id: str,\n",
    "    repo_name: str,\n",
    "    day_start: datetime,\n",
    "    day_end: datetime,\n",
    "    filter_user: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch commits for a single day range [day_start, day_end) with pagination.\n",
    "    Then, for each commit, fetch commit details and summarize.\n",
    "    Returns a DataFrame of commit summaries.\n",
    "    \"\"\"\n",
    "\n",
    "    from_str = day_start.isoformat() + \"Z\"\n",
    "    to_str = day_end.isoformat() + \"Z\"\n",
    "\n",
    "    email_filter = USER\n",
    "\n",
    "    if filter_user:\n",
    "        url = (\n",
    "            f\"{BASE_URL}/{repo_id}/commits?\"\n",
    "            f\"searchCriteria.fromDate={from_str}\"\n",
    "            f\"&searchCriteria.toDate={to_str}\"\n",
    "            f\"&searchCriteria.$top=1000\"\n",
    "            f\"&searchCriteria.author={email_filter}\"\n",
    "            f\"&api-version=7.1\"\n",
    "        )\n",
    "    else:\n",
    "        url = (\n",
    "            f\"{BASE_URL}/{repo_id}/commits?\"\n",
    "            f\"searchCriteria.fromDate={from_str}\"\n",
    "            f\"&searchCriteria.toDate={to_str}\"\n",
    "            f\"&searchCriteria.$top=1000\"\n",
    "            f\"&api-version=7.1\"\n",
    "        )\n",
    "\n",
    "    all_commits = []\n",
    "\n",
    "    while url:\n",
    "        async with session.get(url, headers=HEADERS, timeout=10000) as response:\n",
    "            if response.status == 200:\n",
    "                raw_text = await response.text()\n",
    "                if DEBUG:\n",
    "                    with open(\n",
    "                        f\"raw_responses/{repo_name}_{random.randint(0, 1000)}.json\",\n",
    "                        \"w\",\n",
    "                        encoding=\"utf-8\",\n",
    "                    ) as f:\n",
    "                        f.write(f\"{response.headers}\\n{raw_text}\")\n",
    "                data = json.loads(raw_text)\n",
    "\n",
    "                batch_commits = data.get(\"value\", [])\n",
    "                all_commits.extend(batch_commits)\n",
    "                link_header = response.headers.get(\"Link\", \"\")\n",
    "                next_url = None\n",
    "                if link_header and 'rel=\"next\"' in link_header:\n",
    "                    parts = link_header.split(\";\")\n",
    "                    if parts:\n",
    "                        first_part = parts[0].strip()\n",
    "                        if first_part.startswith(\"<\") and first_part.endswith(\">\"):\n",
    "                            next_url = first_part[1:-1]\n",
    "\n",
    "                if next_url and next_url != url:\n",
    "                    url = next_url\n",
    "                else:\n",
    "                    url = None\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Failed to fetch commits for repository '{repo_name}' (ID: {repo_id}) \"\n",
    "                    f\"on {day_start.date()}. Status code: {response.status}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    commit_summaries = []\n",
    "    for commit in all_commits:\n",
    "        author = commit.get(\"author\", {}).get(\"name\", \"unknown_author\")\n",
    "        commit_id = commit.get(\"commitId\")\n",
    "        commit_url = commit.get(\"url\")\n",
    "\n",
    "        if commit_url and commit_id:\n",
    "            async with session.get(commit_url, headers=HEADERS) as response:\n",
    "                if response.status == 200:\n",
    "                    commit_data = await response.json()\n",
    "                    if commit_data:\n",
    "                        changes_url = (\n",
    "                            commit_data.get(\"_links\", {}).get(\"changes\", {}).get(\"href\")\n",
    "                        )\n",
    "                        if changes_url:\n",
    "                            changes_url = f\"{changes_url}?includeDiff=true\"\n",
    "                            async with session.get(\n",
    "                                changes_url, headers=HEADERS\n",
    "                            ) as changes_resp:\n",
    "                                if changes_resp.status == 200:\n",
    "                                    changes_data = await changes_resp.json()\n",
    "                                    commit_data[\"changes\"] = changes_data\n",
    "\n",
    "                        commit_data[\"summary\"] = summarize_commit(commit_data)\n",
    "                        commit_data[\"summary\"][\"repoName\"] = repo_name\n",
    "                        if DEBUG:\n",
    "                            with open(\n",
    "                                f\"commits/{author}_{commit_id}.json\",\n",
    "                                \"w\",\n",
    "                                encoding=\"utf-8\",\n",
    "                            ) as f:\n",
    "                                f.write(json.dumps(commit_data, indent=4))\n",
    "\n",
    "                        commit_summaries.append(commit_data[\"summary\"])\n",
    "\n",
    "    if commit_summaries:\n",
    "        return pd.DataFrame(commit_summaries)\n",
    "    else:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"commitId\", \"authorName\", \"comment\", \"repoName\", \"date\"]\n",
    "        )\n",
    "\n",
    "\n",
    "async def fetch_commits_for_repo(\n",
    "    session: aiohttp.ClientSession,\n",
    "    repo_id: str,\n",
    "    repo_name: str,\n",
    "    start_date: datetime,\n",
    "    end_date: datetime,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Orchestrate day-by-day async calls to fetch commits for an entire date range.\n",
    "    Returns a combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    tasks = []\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=1)\n",
    "        task = asyncio.create_task(\n",
    "            fetch_commits_for_day(\n",
    "                session,\n",
    "                repo_id,\n",
    "                repo_name,\n",
    "                current_date,\n",
    "                next_date,\n",
    "                filter_user=FILTER_USER,\n",
    "            )\n",
    "        )\n",
    "        tasks.append(task)\n",
    "        current_date = next_date\n",
    "\n",
    "    daily_dfs = await asyncio.gather(*tasks)\n",
    "\n",
    "    if daily_dfs:\n",
    "        return pd.concat(daily_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    repo_id_name_map = await fetch_repositories(session)\n",
    "    all_repo_ids = list(repo_id_name_map.keys())\n",
    "    print(f\"Found {len(all_repo_ids)} repositories.\")\n",
    "\n",
    "    tasks = [\n",
    "        fetch_commits_for_repo(\n",
    "            session, repo_id, repo_id_name_map[repo_id], START_DATE, END_DATE\n",
    "        )\n",
    "        for repo_id in all_repo_ids\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    data = pd.concat(results)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = data.infer_objects()\n",
    "    data[\"changedFilesCount\"] = data[\"changedFilesCount\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [file for sublist in data[\"files\"] for file in sublist]\n",
    "\n",
    "unique_files = set(all_files)\n",
    "\n",
    "distinct_file_count = len(unique_files)\n",
    "\n",
    "print(\"Total distinct files:\", distinct_file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exploded = data.explode(\"files\")\n",
    "\n",
    "distinct_files_per_repo = (\n",
    "    data_exploded.groupby(\"repoName\")\n",
    "    .agg(distinctFileCount=(\"files\", \"nunique\"))\n",
    "    .sort_values(by=\"distinctFileCount\", ascending=False)\n",
    ")\n",
    "\n",
    "distinct_files_per_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"repoName\").agg(\n",
    "    {\"commitId\": \"nunique\", \"changedFilesCount\": \"sum\"}\n",
    ").sort_values(by=\"commitId\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([\"authorName\"]).agg(\n",
    "    {\"changedFilesCount\": \"sum\", \"commitId\": \"nunique\", \"repoName\": \"nunique\"}\n",
    ").sort_values(\"commitId\", ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
